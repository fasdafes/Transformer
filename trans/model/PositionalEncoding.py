import torch
import torch.nn as nn
import torch.nn.functional as F
import math

"""
Positional Encoding lets the model know where elements are supposed to be
by giving them sequences  
PE(2i) = sin(POS/10000^(2i/embed_dim))
PE(2i+1) = cos(POS/10000^(2i/embed_dim))

posï¼šåºåˆ—ä¸­çš„ä½ç½®ï¼Œä» 0 å¼€å§‹é€’å¢ï¼ˆè¡¨ç¤ºæ˜¯ç¬¬å‡ ä¸ª tokenï¼‰
iï¼šç¼–ç ç»´åº¦ç´¢å¼•ï¼ˆembedding çš„ç¬¬å‡ ä¸ªç»´åº¦ï¼‰
2iã€2i+1ï¼šè¡¨ç¤ºåˆ†åˆ«ç”¨ sin å’Œ cos äº¤é”™å¡«å……å‘é‡
10000^(2ğ‘–/embed_dim)
 ï¼šè°ƒèŠ‚ä¸åŒç»´åº¦çš„é¢‘ç‡ï¼Œä½ç»´é¢‘ç‡é«˜ï¼Œé«˜ç»´é¢‘ç‡ä½
"""
class PositionalEncoding(nn.Module):
    def __init__(self,embed_dim,max_len = 5000):
        super().__init__()

        pe = torch.zeros(max_len,embed_dim)
        position = torch.arange(0,max_len,dtype=torch.float).unsqueeze(1)
        divide = torch.exp(torch.arange(0,embed_dim,2).float()*(-math.log(10000.0)/embed_dim))

        pe[:,0::2] = torch.sin(position *divide)
        pe[:,1::2] = torch.cos(position *divide)

        pe = pe.unsqueeze(0)
        self.register_buffer('pe',pe)

    def forward(self,x):
        seq_len = x.size(1)
        return x+self.pe[:,:seq_len]